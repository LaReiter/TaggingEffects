---
title: 'Narwhals'
author: 'Lars Reiter Nielsen'
output:
    html_document:
      css: styles.css
      highlight: pygments
      code_folding: show
      fig_height: 5
      fig_width: 10
      theme: readable
#    toc: yes
#  pdf_document:
#    toc: yes

---

```{r init,echo=F,warning=F,include=F}
library(ggplot2)   ## Grammar of graphics
library(reshape2)  ## Reshaping data frames
library(dplyr)     ## 
library(data.table) ##
library(readr)
library(lattice)   ## More graphics
library(hexbin)    ## and more graphics
library(gridExtra) ## ... and more graphics
library(xtable)    ## LaTeX formatting of tables
library(splines)   ## Splines -- surprise :-)
library(grid)      ## For 'unit'
library(latex2exp)
library(tinytex)
library(readxl)
library(stringr)
library(segmented) # segmented regression
library(tseriesEntropy)
theme_set(theme_bw())

```


# Metode: Depth diagrams and Jensen-Shannon/Kulback-Leibler


```{r data klargøring, echo=FALSE}

#### Load data ####
df <- read.table('Finalmin_BW.txt', sep="\t",header=TRUE)
df <- df %>% dplyr::select(min, Depth,Ind) # ignore depth column
dt <- as.data.table(df) # data table format

# remove ting3
dt<- dt[dt$Ind!="Ting3"]

#### Initialize data ####

depthInt <- 10

# set handling time and sex
dt<- mutate(dt,
            depthlvl = floor(abs(dt$Depth)/depthInt),  # depth level = interval of 20
            TD = rep(0,nrow(dt)),
            TDD = rep(0,nrow(dt)))


#### add "Target Depth" (TD) and "Target Depth Duration" (TDD) ####

#' Remove consecutive dublicates of depthlvl. 
#' X and Y contains two elements (values = Depthlvl)
#' and (lengths = Occurences of values)
X <- rle(dt$depthlvl) 

# IMPORTANT: Ensure that X starts and end in a dive, otherwise the lengths of dummy1 and 2 should be corrected
maxdepthlvl <- max(X$values)
surfaceidx <- which(X$values %in% c(0))

# The dummy variables depend upon the TAIL and FRONT (if we start/end at surface)
front.idx <- X$values[1]>0
tail.idx <- tail(X$values,1)>0
dummy.L <- 2*length(surfaceidx)+1 # dummy length
if(tail.idx != front.idx ){dummy.L <- 2*length(surfaceidx)} 
# ILLUSTRATION of the principle on the 4 lines above:
#   0 1 0 1 0 
#   1 0 1 0 1
#   0 1 0 1
#   1 0 1 0 
# Here 0 represents "surface" and 1 represents "dive". Hence if we are at surface at the start
# and dive in the end or vice versa, the length is only 2*surface length.
dummy1 <- rep(0,dummy.L) # initiate dummy for TD
dummy2 <-rep(0,dummy.L) # initiate dummy for TDD



# Dummy variable for Target Depth
if (front.idx == 0){
  dummy1[1] <- 0
  for(i in 2:(length(surfaceidx))){
    # get max depth between two consecutive surface lvls
    dummy1[2*(i-1)] <- max(X$values[surfaceidx[i-1]:surfaceidx[i]]) 
  }
}

if (front.idx == 1){
  dummy1[1] <- max(X$values[1:surfaceidx[1]])
  for(i in 2:(length(surfaceidx))){
    # get max depth between two consecutive surface lvls
    dummy1[2*i-1] <- max(X$values[surfaceidx[i-1]:surfaceidx[i]]) 
  }
}

if (tail.idx==1) dummy1[dummy.L] <- max(X$values[surfaceidx[length(surfaceidx)]:length(X$values)])


# Dummy variable for Dive Duration
dummy2[1] <- X$lengths[1] # initiate
m <- 1 # m is updated according to which "dive" we are inside

# loop over X lengths, and update accordingly
for(k in 1:length(X$lengths)){
    # if we are at the surface, add the numbers of surface ticks
    if(X$values[k]==0){      
     if (k!=1){m <- m+1} # new "surface dive"
     dummy2[m] <- X$lengths[k]
     m <- m+1 # begin new "other" dive
    }
    # else, we are diving and we add the length of the specific depth to TDD
    else{ 
      dummy2[m] <- dummy2[m] + X$lengths[k]
    }
  }
    
# update target depth and target depth duration
# nb: -1 to adjust for the 1st minute missing (remove 1st element)
dummy2[1]<-dummy2[1]-1
dt$TD <- rep(dummy1,dummy2)
dt$TDD <- rep(dummy2,dummy2)

shallow.th <- floor(60/depthInt) # SHALLOW dives are defined to be between 20 and 8*6 = 48 meters
medium.th <- floor(120/depthInt) # MEDIUM dives are defined to be between 60 and 20*6 = 120 meters

dt <- mutate(dt, dive = ifelse(TD==0,"surface",
                               ifelse(TD < shallow.th, "shallow",
                                                ifelse(TD < medium.th, "medium", "deep"))),
             dummyS = ifelse(TD==0,1/TDD,0), # for computing surface per hour
             dummyShallow = ifelse(TD >0 & TD <= shallow.th, 1/TDD,0), # for computing shallow dives per hour
             dummyMedium = ifelse(TD > shallow.th & TD <= medium.th, 1/TDD,0),
             dummyDeep = ifelse(TD>medium.th,1/TDD,0)) # for computing deep dives per hour

# remove unused columns
dt[,Depth:=NULL]

# length (minutes - 1) of observations for all whales
ll <- aggregate(dt$min,list(dt$Ind),length)$x+1 # actual length
lu <- cumsum(ll+60-ll %% 60) # extended length (if ll should be a multiple of 60)

# the below vector is used to remove overflow of observations
l <- c(ll[1]:lu[1])
for(i in 2:length(ll)) l <- c(l,(lu[i-1]+ll[i]):lu[i])

# vector to be used later (corrected to the actual numbers of observations)
vecDummy <- rep(seq(1, nrow(dt)), each = 60)
vecDummy<-vecDummy[-l] # remove overflow observations
vecDummy <- vecDummy[1:nrow(dt)] # chop at tail


# COUNT the number of shallow dives every 60 min (shallow dives per hour)
dt$shallow.count <- ave(dt$dummyShallow, vecDummy, 
                        FUN = function(x) sum(x))
dt$shallow.time <- ave(dt$dummyShallow, vecDummy, 
                        FUN = function(x) sum(x>0)/60)

# COUNT the number of medium dives every 60 min (medium dives per hour)
dt$medium.count <- ave(dt$dummyMedium, vecDummy, 
                        FUN = function(x) sum(x))
dt$medium.time <- ave(dt$dummyMedium, vecDummy, 
                        FUN = function(x) sum(x>0)/60)


# COUNT the number of deep dives every 60 min (deep dives per hour)
dt$deep.count <- ave(dt$dummyDeep, 
                        vecDummy, 
                        FUN = function(x) sum(x))
dt$deep.time <- ave(dt$dummyDeep, 
                        vecDummy, 
                        FUN = function(x) sum(x>0)/60)

# remove unused columns
dt[,dummyDeep:=NULL]
dt[,dummyShallow:=NULL]   

# create hourly data frame
dt.H <-aggregate(list(Ind=dt$Ind,
                      shallow.count=dt$shallow.count,
                      shallow.time=dt$shallow.time,
                      medium.count=dt$medium.count,
                      medium.time=dt$medium.time,
                      deep.count = dt$deep.count,
                      deep.time =dt$deep.time),
                 by=list(hours=vecDummy),
                 FUN=function(x) unique(x))

wh <- rle(dt.H$Ind)$lengths # whale hours
lw <- c(1:wh[1])
for(i in 2:length(wh)) lw <- c(lw,1:wh[i])
dt.H$hours <- lw # update whale hours (correct format)

## OPTIONAL: We can now choose to center our new depth variables with the long time average
## where long time average is above 30*60 minutes (30 hours)

#long.df <- dt[dt$min>60*30,] %>% group_by(Ind) %>% summarize(deep.long=mean(deep.count/TDD)*60,
#                                                             shallow.long = #mean(shallow.count/TDD)*60)
#for(w in long.df$Ind){
#  dt[dt$Ind==w,]$shallow.count <- dt[dt$Ind==w,]$shallow.count - #long.df[long.df$Ind==w,]$shallow.long
#  dt[dt$Ind==w,]$deep.count <- dt[dt$Ind==w,]$deep.count - long.df[long.df$Ind==w,]$deep.long
#}


```



```{r echo=FALSE}
#### GLOBAL SETTINGS ####

# Remove whales with sample duration < 40hrs

# RESTRICT to data less than th hours (otherwise we lose too many whales in the tail)
#th <- 25 # set this to 57?
#dt <- dt[dt$min < th*60+2,] # + 2 is because we start at 2nd minute


###########################################################
#### SET TIME/TD INTERVAL LENGTH AND TIME/TD INTERVALS ####
###########################################################

M <-1 # TIME interval length
nInts <- 10 # number of intervals to use (8 ok)
# THE CHOICE IS BECAUSE THAT WE HAVE WHALES THAT LOSES TAG < 60hrs, THUS THEY ARE PARTLY REPRESENTED
# IN THE DATA
# NOTE: remaining hours after M*nInts is used as normal area. i.e. th - M*nInts is normal sampling area.

#N <- 4 # number of quantiles
#qts <- quantile(dt[dt$min>M*nInts & dt$TD!=0,]$TD,1/N*1:N) # normal area quantiles for all whales
md <- max(dt$TD) # max target depth (for all whales!)
# OR set custom Quantiles (Shallow,Medium, Deep)
qts <- c(1,3,6,9) # 6,13, md if using depthInt = 6
N<-length(qts)


correction <- 0 # probability to add at each TD in tail/normal area

ggplot(dt[dt$TD!=0,],aes(min,TD))+
  ggtitle("Target Depth evolution for all whales")+
  geom_point(color="cornflowerblue",alpha=I(0.3))+
  geom_line(aes(x=M*nInts*60),color="brown3",size=1)+
  scale_x_continuous("hours", breaks=600*0:8, labels=paste(10*0:8))+
  scale_y_continuous("Target depth (meters)", breaks=seq(0,md,2), labels=20*seq(0,md,2))+
  theme(legend.position="bottom",plot.title=element_text(hjust=0.5))
ggsave("TDevol-BW.pdf",width=10,height=5.5,dpi=600)
```


Undersøger hvordan dive duration (tid for ét dyk) fordelingen ændrer sig med tiden:

#### SETUP ####

```{r echo=FALSE, fig.height=12, fig.width=14}
th <- nInts*M

getDTB <- function(DTB){
  
  thh <- ceiling(th/M)
  # create dataframe skeleton
  dfD <- data.frame(Interval=rep(1:(thh),each=md),
                  TD=rep(1:md,(thh)),
                  TDD=rep(0,md*(thh)))

  # Aggregate into TIME intervals
  for(i in 1:thh){
    
    if(length(DTB[DTB$min>=(i-1)*M*60 
                              & DTB$min<i*M*60 & DTB$TD!=0]$TDD)!=0){
    t <-aggregate(list(TDD=DTB[DTB$min>=(i-1)*M*60 
                              & DTB$min<i*M*60 & DTB$TD!=0]$TDD),
                  list(TD=as.integer(DTB[DTB$min>=(i-1)*M*60 
                                      & DTB$min<i*M*60 
                                      & DTB$TD!=0]$TD)),
                  sum)
  
    # get probability at each TD cell
    dfD[dfD$Interval==i,][t$TD,"TDD"] <-t$TDD/sum(t$TDD)      
    }
    else{
    dfD[dfD$Interval==i,][,"TDD"] <-0
    }

  }


  # Aggregate into TD intervals (and get every M TD measurement instead):
  # dummy vector for aggregating into non-uniform TD   intervals
  s<-(shift(c(0,qts[2:N]),-1)-c(0,qts[2:N]))[1:(N-1)] 
  dfD2 <- dfD
  dfD2$dummy <- rep(rep(qts[-1],s),thh)
  dfD2 <- aggregate(list(TDD=dfD2$TDD),by=list(Interval=dfD2$Interval, TD.Interval = dfD2$dummy),sum)
  
  return(dfD2)
}

#DT.all <- getDTB(dt)

```

# Jensen-Shannon divergens

![Caption for the picture.](jsd.png)

$P \equiv P_t$ er dive duration fordelingen til tidsintervallet $t$. $Q$ er fordelingen i normalområdet.


## Cross validation / Leave-one-out til estimation af 95% CI for normalområde

```{r echo=F,message=F,warning=F}
#### CROSS VALIDATION SETUP ####

#######################
#### INITIAL SETUP ####
#######################

# create dataframe skeleton for DEPTH GRID
DT.all <- data.frame(Interval=rep(1:(th),each=(N-1)),
                  TD=rep(1:(N-1),(th)),
                  TDD=rep(0,(N-1)*(th)))

Nt <- nInts # hypothesized normal area
dt.N <- dt[dt$min>Nt*M*60 & dt$TD!=0] # Target depth in hypothesized normal area (excluding surface)

## Auxillary function 1: Map TD and TD-quantile intervals
qMap <- Vectorize(function(x){
  # return the corresponding quantile-interval of TD x
  return(max(qts[2],qts[which(qts-x>=0)[1]]))
})

## Auxilliary function 2: Kullback Leibler
KB <- function(p,q){
  # Kullback Leibler Divergence #
  # if p(X)=0,then KB contribution is zero (per definition; see
  # wiki and Kullback Leibler Definition):
  sum(p*log(p/q),na.rm=T)
} 

# All whales
tdrop <- sort(round(aggregate(df[df$min>nInts*M*60,]$min,list(df[df$min>nInts*M*60,]$Ind),function(x)length(x)/60)$x))+nInts*M
Nwhales <- length(unique(dt$Ind)) # number of whales
nt <- Nwhales + 1 - cumsum(rle(tdrop)$lengths) # number of whales before each drop point
tdrop <- rle(tdrop)$values # remove dublicates
tdist <- tdrop-shift(tdrop,1) # distance between drops
tdist[1] <-tdrop[1]-nInts*M+1
tdistrel <- tdist/sum(tdist) # relative distance

# Susanne weighting:
Qweights <- nt/sum(nt)

# My weighting
#Qweights <- nt**3*tdistrel # weight(t) = nt^3*tdist
#Qweights <- Qweights/sum(Qweights)

########################################################
#### CONSTRUCT the expected "normal" distribution Q ####
########################################################

# original Q (no moving window: disjoint cross validation)
constructQ.org <- function(dtIn,R=0){
  # Dataframe and R (boolean) for random sampling where
  # we exclude a random subset of size M if R =1
  
  ndrops <- length(tdrop)
  Qmat <- matrix(0,nrow=N-1,ncol=ndrops) # initiate Q at different (weighted) areas
  tmax <- max(tdrop) # last whale dropped
  dummyvec <- rep(0,ndrops)
  tdist.dummy <- tdist 
  
  if(R<0)tskip <- sample(Nt:(tmax),1)
  if(R>0)tskip <- R
  
  # correct normalization after removal of one hour
  idxRemove <- ndrops+1-sum(tskip<=tdrop)
  tdist.dummy[idxRemove] <- tdist.dummy[idxRemove]-1
  
  Q <- rep(0,length(qts)-1)
  for(t in Nt:(tmax)){
    
    if(t!=tskip){ # if not the random subset that we exclude
      
      # get relevant area
      idx <- ndrops+1-sum(t<=tdrop)
      
      if(length(dtIn[dtIn$min>=(t-1)*M*60 & dtIn$min<t*M*60 & dtIn$TD!=0]$TD)!=0){
  
      t.N <-aggregate(list(TDD=dtIn[dtIn$min>=(t-1)*M*60 
                                & dtIn$min<t*M*60 & dtIn$TD!=0]$TDD),
                    list(TD=as.integer(dtIn[dtIn$min>=(t-1)*M*60 
                                       & dtIn$min<t*M*60 
                                        & dtIn$TD!=0]$TD)),
                    sum)
  
      # aggregate over TD quantile intervals (and add missing intervals)
      t.N <- aggregate(list(TDD=t.N$TDD),list(TD.interval=qMap(t.N$TD)),sum)
      t.N0 <- data.frame(TD.interval = qts[-1],TDD=0)
      t.N0[which(qts[-1] %in% t.N$TD.interval),]$TDD <- t.N$TDD

      # add to Q for that area
      Qmat[1:(N-1),idx] <- Qmat[1:(N-1),idx]+t.N0$TDD/sum(t.N0$TDD)
      
      }
      else{Qmat[1:(N-1),idx] <-  Qmat[1:(N-1),idx] + rep(0,(N-1))}
     }
  
  }
  
  Qmat <- t(replicate(N-1,Qweights))*Qmat/t(replicate(N-1,tdist.dummy)) # normalize
  return(rowSums(Qmat))
}




##################################
#### CALCULATE Jensen-Shannon ####
##################################


tmax<-floor(max(dt$min)/60) # max sampling time
JS.all <- rep(0,tmax)

for(t in 1:tmax){
  
  print(t)
  
  Q<-constructQ.org(dt,-1) # construct Q by removing a K subset randomly
  if(t>=Nt){Q<-constructQ.org(dt,t)}
  
  if(length(dt[dt$min>=(t-1)*M*60 & dt$min<t*M*60 & dt$TD!=0]$TD)!=0){
  ### ALL handling:
    t.all <-aggregate(list(TDD=dt[dt$min>=(t-1)*M*60 
                                & dt$min<t*M*60 & dt$TD!=0]$TDD),
                    list(TD=as.integer(dt[dt$min>=(t-1)*M*60 
                                        & dt$min<t*M*60 
                                        & dt$TD!=0]$TD)),
                    sum)
    
    # aggregate over TD quantile intervals (and add missing intervals)
    t.all <- aggregate(list(TDD=t.all$TDD),list(TD.interval=qMap(t.all$TD)),sum)
    t.all0 <- data.frame(TD.interval = qts[-1],TDD=0)
    t.all0[which(qts[-1] %in% t.all$TD.interval),]$TDD <- t.all$TDD
    
    # Handling the kth subset
    P.all <- t.all0$TDD/sum(t.all0$TDD)
  }
  else
  {
    P.all <- rep(0,N)
  }
  
  if(t<th) DT.all[DT.all$Interval==t,]$TDD <- P.all
  
  # calculate JSD
  MM <- 1/2*(P.all+Q)
  JS.all[t] <- 1/2*KB(P.all,MM)+1/2*KB(Q,MM)
  
}

# add reference dist
DT.all[DT.all$Interval==th,]$TDD <- constructQ.org(dt,-1)

# Normal area Jensen-Shannon (combine)
trim.max <- 16
JS.N <- JS.all[Nt:trim.max]

```

# Dybde diagrammer

```{r}
#### PLOT: Combined Dataframe ####

#intLabsx <- c(sapply(seq(1,nInts*M-1,M),function(x) paste0("[",x,",",x+M,"[") ),"Normal")
intLabsx <- c(1:(nInts*M-1),"Normal")
intLabsy <- mapply(function(X,Y)paste0("[",X,",",Y,"[") ,qts[-N]*20,shift(qts,-1)[-N]*20)
names(intLabsy)<-NULL
names(qts)<- NULL

ggplot(DT.all,aes(x=as.factor(Interval),y=as.factor(TD),fill=TDD)) +
 geom_tile(aes(fill=TDD),color="black") +
#  geom_text(aes(label = round(TDD,2)), color = "black", size = 4)+
  coord_fixed()+
  xlab("hours")+
  ylab("Target depth (meters)")+
  scale_fill_gradient(low = "white", high = "goldenrod",name="Time spend (%)")+
  scale_x_discrete(breaks = 1:(nInts),labels=intLabsx)+
  scale_y_discrete(breaks=as.factor(1:(N-1)),labels=intLabsy)+
  theme(axis.text.x = element_text(angle = 45, vjust = 1.2, hjust=1),
        plot.title =element_text(hjust = 0.5,vjust=3.5))+
  ggtitle("Hourly emperical distribution(s) of Dive Duration for N=4 bowhead whales")

#  ggsave("DepthGrid-BW.pdf",dpi=600) # width 10 height 8

save(DT.all, file="BWDG.RData")
```


```{r}
##########################################################
### METHOD 1: BLOCK BOOTSTRAPPING SIMPLE #################
##########################################################
th <- Nt

eps <- 0
alpha.top <- quantile(JS.N,0.975,na.rm = T)
alpha.bottom <- quantile(JS.N,0.025,na.rm =T)

# Function to check if inside or outside CI
plusMinus <- Vectorize(function(x){ifelse(x>alpha.top+eps | x<alpha.bottom-eps,-1,1)}  )

N.sims <- 200 # number of sims
lB <- round(th^(1/3)) # length of block
B <- floor(th/lB) # number of blocks
t.max <- floor(th/lB)*lB
time.hrs <- 1:t.max # time

# breakpoints
BP.A <- rep(0,N.sims)

# sample for graphical example
exN <- 2
exdf <- data.frame(hours=rep(time.hrs,exN),votes=NA,ID=rep(1:exN,each=t.max))

for(n in 1:N.sims){
  # case resampling within each block
  V.bs.A <- rep(0,t.max)
  for(b in 0:(B-1)){
    idx <- (b*lB+1):((b+1)*lB) # indices
    V.bs.A[idx] <- sample(plusMinus(JS.all[idx])) # block sampling
  }  
  # make dataframes
  datf.A <-data.frame(votes=c(0,cumsum(V.bs.A)),hours=c(0,time.hrs))
  
  # save graphical example
  if(n <= exN){
    exdf[exdf$ID==n,]$votes <- cumsum(V.bs.A)
  }
  
  # compute breakpoints using segmented regression
  lm.A <- lm(votes~hours,data=datf.A)
  sm.A <- segmented(lm.A,npsi=1,control = seg.control(random=F))
  
  BP.A[n] <- ifelse(is.null(sm.A$psi[2]),1,sm.A$psi[2])
}

# plot graphical examples
ggplot(exdf, aes(hours,votes,color=factor(ID)))+
  geom_line(size=1)+
  ggtitle("Block bootstrap samples (3 examples)")

ggsave("JS-votes-samples-BW.pdf",width=10,height=6,dpi=600)

# statistics
meanRec <- mean(BP.A)
ciRec <- quantile(BP.A,c(0.025,0.975))


```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# All only
df.JS <- data.frame(Interval=rep(1:trim.max,1),
                    Handlingtime=rep(c("All (15)"),each=trim.max),
                    JS=c(JS.all[1:trim.max]))


ggplot(df.JS,aes(x=Interval,y=JS))+
geom_ribbon(aes(ymin=quantile(JS.N,0.025) ,alpha=I(0.1),ymax =quantile(JS.N,0.975)),color="black",fill="gold")+
  geom_point(size=2,shape=24,fill="khaki3")+
  geom_vline(aes(xintercept=ciRec[1]),linetype="solid",color="gray12")+
  geom_vline(aes(xintercept=meanRec),linetype="longdash",color="gray12")+
  geom_vline(aes(xintercept=ciRec[2]),linetype="solid",color="gray12")+
  geom_hline(aes(yintercept=mean(JS.N)),linetype="longdash")+
  ylab("Jensen-Shannon divergence")+
  xlab("hours")+
  scale_x_continuous(breaks=seq(0,trim.max,max(1,floor(5/M))),
                     labels=M*seq(0,trim.max,max(1,floor(5/M))),
                     limits=c(1,trim.max))+
 scale_color_manual(values=c("blue", "red"))+
  scale_fill_manual(values=c("royalblue", "brown1"))+
  labs(title = str_wrap("Cross entropy of hourly Dive duration post-tagging relative to normal (> 10 hours) Dive duration for N=4 bowhead whales", 60))+
  theme(legend.position="bottom",plot.title=element_text(hjust=0.5))
ggsave("JS-BW.pdf",width=10,height=6,dpi=600)


```



```{r}
############################################
#### USE VOTING TO DETERMINE BREAKPOINT ####
############################################

alpha.top <- quantile(JS.N,0.975)
alpha.bottom <- quantile(JS.N,0.975)
V.all <- data.frame(hours=1:t.max,Votes=rep(0,t.max),HT="All",Vpred=NA,Vu=NA,Vl=NA) # Votes

# Initiate
if(JS.all[1]>alpha.top) V.all[1,]$Votes <--1 else V.all[1,]$Votes <-1

for(t in 2:t.max){
  # all
  if(JS.all[t]>alpha.top){V.all[t,]$Votes <- V.all[t-1,]$Votes - 1}
  else{V.all[t,]$Votes <- V.all[t-1,]$Votes +1}
}

```

```{r}
save(df.JS,file="BW.JS.RData")
save(JS.N,file="BW.JSN.RData")
avec <- c(meanRec,ciRec)
save(avec,file="BW.votes.RData")
```


